---
title: 'MASIL: Towards Maximum Separable Class Representation for Few Shot Class Incremental
  Learning'
abstract: Few Shot Class Incremental Learning (FSCIL) with few examples per class
  for each incremental session is the realistic setting of continual learning since
  obtaining large number of annotated samples is not feasible and cost effective.
  We present the framework MASIL as a step towards learning the maximal separable
  classifier. It addresses the common problem i.e forgetting of old classes and over-fitting
  to novel classes by learning the classifier weights to be maximally separable between
  classes forming a simplex Equiangular Tight Frame. We propose the idea of concept
  factorization explaining the collapsed features for base session classes in terms
  of concept basis and use these to induce classifier simplex for few shot classes.
  We further adds fine tuning to reduce any error occurred during factorization and
  train the classifier jointly on base and novel classes without retaining any base
  class samples in memory. Experimental results on miniImageNet, CIFAR-100 and CUB-200
  demonstrate that MASIL outperforms all the benchmarks.
openreview: adSACexIvw
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: khandelwal23a
month: 0
tex_title: 'MASIL: Towards Maximum Separable Class Representation for Few Shot Class
  Incremental Learning'
firstpage: 519
lastpage: 533
page: 519-533
order: 519
cycles: false
bibtex_author: Khandelwal, Anant
author:
- given: Anant
  family: Khandelwal
date: 2023-09-27
address: 
container-title: Proceedings of 2nd Annual Workshop on Topology, Algebra, and Geometry
  in Machine Learning (TAG-ML)
volume: '221'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 9
  - 27
pdf: https://proceedings.mlr.press/v221/khandelwal23a/khandelwal23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
