---
title: 'Sumformer: Universal Approximation for Efficient Transformers'
abstract: Natural language processing (NLP) made an impressive jump with the introduction
  of Transformers. ChatGPT is one of the most famous examples, changing the perception
  of the possibilities of AI even outside the research community. However, besides
  the impressive performance, the quadratic time and space complexity of Transformers
  with respect to sequence length pose significant limitations for handling long sequences.
  While efficient Transformer architectures like Linformer and Performer with linear
  complexity have emerged as promising solutions, their theoretical understanding
  remains limited. In this paper, we introduce Sumformer, a novel and simple architecture
  capable of universally approximating equivariant sequence-to-sequence functions.
  We use Sumformer to give the first universal approximation results for Linformer
  and Performer. Moreover, we derive a new proof for Transformers, showing that just
  one attention layer is sufficient for universal approximation.
openreview: 7fCKJnur4u
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: alberti23a
month: 0
tex_title: 'Sumformer: Universal Approximation for Efficient Transformers'
firstpage: 72
lastpage: 86
page: 72-86
order: 72
cycles: false
bibtex_author: Alberti, Silas and Dern, Niclas and Thesing, Laura and Kutyniok, Gitta
author:
- given: Silas
  family: Alberti
- given: Niclas
  family: Dern
- given: Laura
  family: Thesing
- given: Gitta
  family: Kutyniok
date: 2023-09-27
address: 
container-title: Proceedings of 2nd Annual Workshop on Topology, Algebra, and Geometry
  in Machine Learning (TAG-ML)
volume: '221'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 9
  - 27
pdf: https://proceedings.mlr.press/v221/alberti23a/alberti23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
