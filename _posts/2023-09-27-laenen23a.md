---
title: One-Shot Neural Network Pruning via Spectral Graph Sparsification
abstract: Neural network pruning has gained significant attention for its potential
  to reduce computational resources required for training and inference. A large body
  of research has shown that networks can be pruned both after training and at initialisation,
  while maintaining competitive accuracy compared to dense networks. However, current
  methods rely on iteratively pruning or repairing the network to avoid over-pruning
  and layer collapse. Recent work has found that by treating neural networks as a
  sequence of bipartite graphs, pruning can be studied through the lens of spectral
  graph theory. Therefore, in this work, we propose a novel pruning approach using
  spectral sparsification, which aims to preserve meaningful properties of a dense
  graph with a sparse subgraph, by preserving the spectrum of the dense graphâ€™s adjacency
  matrix. We empirically validate and investigate our method, and show that one-shot
  pruning using spectral sparsification preserves performance at higher levels of
  sparsity compared to its one-shot counterparts. Additionally, we theoretically analyse
  our method with respect to local and global connectivity.
openreview: XolCjOOeRy
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: laenen23a
month: 0
tex_title: One-Shot Neural Network Pruning via Spectral Graph Sparsification
firstpage: 60
lastpage: 71
page: 60-71
order: 60
cycles: false
bibtex_author: Laenen, Steinar
author:
- given: Steinar
  family: Laenen
date: 2023-09-27
address: 
container-title: Proceedings of 2nd Annual Workshop on Topology, Algebra, and Geometry
  in Machine Learning (TAG-ML)
volume: '221'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 9
  - 27
pdf: https://proceedings.mlr.press/v221/laenen23a/laenen23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
