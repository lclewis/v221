---
title: Metric Space Magnitude and Generalisation in Neural Networks
abstract: Deep learning models have seen significant successes in numerous applications,
  but their inner workings remain elusive. The purpose of this work is to quantify
  the learning process of deep neural networks through the lens of a novel topological
  invariant called magnitude. Magnitude is an isometry invariant; its properties are
  an active area of research as it encodes many known invariants of a metric space.
  We use magnitude to study the internal representations of neural networks and propose
  a new method for determining their generalisation capabilities. Moreover, we theoretically
  connect magnitude dimension and the generalisation error, and demonstrate experimentally
  that the proposed framework can be a good indicator of the latter.
openreview: lwjoFxFxZQ
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: andreeva23a
month: 0
tex_title: Metric Space Magnitude and Generalisation in Neural Networks
firstpage: 242
lastpage: 253
page: 242-253
order: 242
cycles: false
bibtex_author: Andreeva, Rayna and Limbeck, Katharina and Rieck, Bastian and Sarkar,
  Rik
author:
- given: Rayna
  family: Andreeva
- given: Katharina
  family: Limbeck
- given: Bastian
  family: Rieck
- given: Rik
  family: Sarkar
date: 2023-09-27
address: 
container-title: Proceedings of 2nd Annual Workshop on Topology, Algebra, and Geometry
  in Machine Learning (TAG-ML)
volume: '221'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 9
  - 27
pdf: https://proceedings.mlr.press/v221/andreeva23a/andreeva23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
